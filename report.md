# REPORT - LAB 9: ULTIMATE PRACTICE
## M√îN: NH·∫¨P M√îN K·ª∏ THU·∫¨T D·ªÆ LI·ªÜU - L·ªöP: DHKHDL19A
## Danh s√°ch th√†nh vi√™n:
>> 1. B√πi Quang Th√†nh
>> 2. Nguy·ªÖn Th·ªã Ph∆∞∆°ng Th·∫£o
>> 3. Tr∆∞∆°ng ƒê·∫∑ng Ho√†ng Tuy·∫øn

# B√ÄI L√ÄM
> 1. ƒêƒÉng nh·∫≠p v√†o t√†i kho·∫£ng Github

> 2. Truy c·∫≠p v√†o link:
> 
> 3. Ch·ªçn fork
![image](https://github.com/user-attachments/assets/78bc5f7e-3354-46d3-93ae-37df89da9613)

> 4. Click Create fork
![image](https://github.com/user-attachments/assets/bf0a8369-8568-401f-a337-7457d2c25a3b)

## EXERCISE 1

> 1. Th·ª±c thi l·ªánh sau trong CMD: git clone ƒë·ªÉ clone GitHub repo v·ªÅ m√°y c·ªßa m√¨nh
![image](https://github.com/user-attachments/assets/03a1821e-dd25-4748-931d-afee36e47d7e).

> 2. Sau ƒë√≥ ti·∫øn h√†nh ch·∫°y l·ªánh `cd data-engineering-practice/Exercises/Exercise-1` ƒë·ªÉ thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c Exercise-1

> 3. Ti·∫øp t·ª•c th·ª±c hi·ªán l·ªánh: `docker build --tag=exercise-1 .` ƒë·ªÉ build Docker image Qu√° tr√¨nh s·∫Ω m·∫•t v√†i ph√∫t
![70fb32a899772b297266](https://github.com/user-attachments/assets/250e5f8f-4bf3-4263-b32d-9832969553f4)
![3c90cf92794dcb13925c](https://github.com/user-attachments/assets/9e3bd508-8290-41bf-8e1a-6223464211c3)
![7a5be3c75018e246bb09](https://github.com/user-attachments/assets/ee05f4d0-72e5-4105-8326-aef28a6f8d41)


> 4. S·ª≠ d·ª•ng Visual ƒë·ªÉ ch·∫°y main.py
![c7929cded400665e3f11](https://github.com/user-attachments/assets/e9934529-5d0d-4e97-985f-69e06a9041eb)


> ##### Code s·ª≠ d·ª•ng cho main.py
```
# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os               # L√†m vi·ªác v·ªõi h·ªá th·ªëng file (t·∫°o th∆∞ m·ª•c, ƒë∆∞·ªùng d·∫´n, x√≥a file)
import requests         # G·ª≠i HTTP request ƒë·ªÉ t·∫£i file t·ª´ Internet
import zipfile          # Gi·∫£i n√©n c√°c file .zip

# Danh s√°ch c√°c URL ch·ª©a d·ªØ li·ªáu c·∫ßn t·∫£i xu·ªëng
download_uris = [
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2018_Q4.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2019_Q1.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2019_Q2.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2019_Q3.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2019_Q4.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2020_Q1.zip",
    "https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2220_Q1.zip",  # URL sai ƒë·ªÉ ki·ªÉm tra b·∫Øt l·ªói
]

# Th∆∞ m·ª•c l∆∞u file sau khi t·∫£i v√† gi·∫£i n√©n
DOWNLOAD_DIR = "downloads"

# H√†m x·ª≠ l√Ω t·∫£i xu·ªëng v√† gi·∫£i n√©n m·ªôt file
def download_and_extract(url):
    # L·∫•y t√™n file t·ª´ URL (vd: Divvy_Trips_2019_Q1.zip)
    filename = url.split("/")[-1]
    zip_path = os.path.join(DOWNLOAD_DIR, filename)  # ƒê∆∞·ªùng d·∫´n l∆∞u file .zip

    try:
        print(f"Downloading: {filename}")

        # G·ª≠i GET request ƒë·∫øn URL
        response = requests.get(url)
        response.raise_for_status()  # N·∫øu l·ªói HTTP (vd: 404, 403) s·∫Ω raise exception

        # Ghi n·ªôi dung t·∫£i ƒë∆∞·ª£c v√†o file .zip
        with open(zip_path, "wb") as f:
            f.write(response.content)

        # M·ªü file .zip v√† gi·∫£i n√©n t·∫•t c·∫£ n·ªôi dung v√†o th∆∞ m·ª•c DOWNLOAD_DIR
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(DOWNLOAD_DIR)

        # X√≥a file .zip sau khi gi·∫£i n√©n th√†nh c√¥ng
        os.remove(zip_path)

        print(f"‚úì Done: {filename}")

    # B·∫Øt l·ªói HTTP (URL l·ªói, kh√¥ng t·ªìn t·∫°i...)
    except requests.exceptions.HTTPError as http_err:
        print(f"‚ùå HTTP error for {filename}: {http_err}")

    # B·∫Øt l·ªói n·∫øu file t·∫£i v·ªÅ kh√¥ng ph·∫£i file zip h·ª£p l·ªá
    except zipfile.BadZipFile:
        print(f"‚ùå Not a valid zip file: {filename}")

    # B·∫Øt c√°c l·ªói kh√°c
    except Exception as e:
        print(f"‚ùå Failed {filename}: {e}")

# H√†m main ƒëi·ªÅu ph·ªëi qu√° tr√¨nh
def main():
    # T·∫°o th∆∞ m·ª•c "downloads" n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)

    # L·∫∑p qua t·ª´ng URL trong danh s√°ch v√† x·ª≠ l√Ω
    for url in download_uris:
        download_and_extract(url)

# Khi ch·∫°y script tr·ª±c ti·∫øp, g·ªçi h√†m main
if __name__ == "__main__":
    main()

```
> ƒêo·∫°n code tr√™n th·ª±c hi·ªán c√°c t√°c v·ª•: 
- T·∫°o th∆∞ m·ª•c downloads n·∫øu ch∆∞a t·ªìn t·∫°i

- T·∫£i t·ª´ng file t·ª´ danh s√°ch download\_uris

- Gi·ªØ t√™n g·ªëc c·ªßa file t·ª´ URL

- Gi·∫£i n√©n .zip th√†nh .csv

- X√≥a file .zip sau khi gi·∫£i n√©n

- B·ªè qua URL kh√¥ng h·ª£p l·ªá (v√≠ d·ª•: c√°i Divvy\_Trips\_2220\_Q1.zip kh√¥ng t·ªìn t·∫°i)

> 5. Sau khi save `main.py`, ch·∫°y l·ªánh `docker-compose up run` (m·∫•t kho·∫£ng 5 ph√∫t)
![bada83a1af7f1d21446e](https://github.com/user-attachments/assets/0c8b5d85-dd88-486c-b4bf-f1f9ccfa7eab)


## EXERCISE 2

> 1. Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c t·∫°i CMD th√†nh `Exercise-2`

> 2. Ch·∫°y l·ªánh docker `build --tag=exercise-2 .` ƒë·ªÉ build image Docker (Qu√° tr√¨nh di·ªÖn ra trong 2 ‚Äì 3 ph√∫t)
> ![1504b98db053020d5b42](https://github.com/user-attachments/assets/734aab65-dc74-4c71-ab0f-d5de198ec074)

> 3. Sau khi build xong, truy c·∫≠p file main.py b·∫±ng VS code


##### N·ªôi dung file main.py

```import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

# URL c·ªßa trang web ch·ª©a c√°c t·ªáp c·∫ßn t·∫£i
BASE_URL = "https://www.ncei.noaa.gov/data/local-climatological-data/access/2021/"

# D·∫•u th·ªùi gian c·∫ßn t√¨m ki·∫øm tr√™n trang web
TARGET_TIMESTAMP = "2024-01-19 10:27"

def find_target_file():
    """
    H√†m n√†y s·∫Ω duy·ªát qua trang web, t√¨m ki·∫øm t·ªáp v·ªõi d·∫•u th·ªùi gian
    TARGET_TIMESTAMP v√† tr·∫£ v·ªÅ t√™n t·ªáp t∆∞∆°ng ·ª©ng.
    """
    response = requests.get(BASE_URL)  # G·ª≠i y√™u c·∫ßu GET t·ªõi trang web
    response.raise_for_status()  # Ki·ªÉm tra n·∫øu y√™u c·∫ßu th√†nh c√¥ng

    soup = BeautifulSoup(response.text, 'lxml')  # Ph√¢n t√≠ch trang HTML

    # T√¨m t·∫•t c·∫£ c√°c d√≤ng trong b·∫£ng
    rows = soup.find_all("tr")

    for row in rows:
        cols = row.find_all("td")  # T√¨m c√°c √¥ trong d√≤ng
        if len(cols) >= 2:
            timestamp = cols[1].text.strip()  # L·∫•y d·∫•u th·ªùi gian
            if timestamp == TARGET_TIMESTAMP:
                filename = cols[0].text.strip()  # L·∫•y t√™n t·ªáp
                return filename  # Tr·∫£ v·ªÅ t√™n t·ªáp n·∫øu t√¨m th·∫•y

    # N·∫øu kh√¥ng t√¨m th·∫•y t·ªáp v·ªõi d·∫•u th·ªùi gian y√™u c·∫ßu
    raise Exception(f"File with timestamp {TARGET_TIMESTAMP} not found.")

def download_file(filename):
    """
    H√†m n√†y s·∫Ω t·∫£i t·ªáp t·ª´ URL v√† l∆∞u t·ªáp v√†o th∆∞ m·ª•c 'downloads'.
    """
    download_url = BASE_URL + filename  # X√¢y d·ª±ng URL ƒë·∫ßy ƒë·ªß ƒë·ªÉ t·∫£i t·ªáp
    local_path = os.path.join("downloads", filename)  # ƒê∆∞·ªùng d·∫´n l∆∞u t·ªáp

    # T·∫°o th∆∞ m·ª•c 'downloads' n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs("downloads", exist_ok=True)

    # G·ª≠i y√™u c·∫ßu GET ƒë·ªÉ t·∫£i t·ªáp
    response = requests.get(download_url)
    response.raise_for_status()  # Ki·ªÉm tra n·∫øu y√™u c·∫ßu th√†nh c√¥ng

    # L∆∞u t·ªáp v√†o h·ªá th·ªëng
    with open(local_path, 'wb') as f:
        f.write(response.content)
    print(f"Downloaded file to {local_path}")  # Th√¥ng b√°o t·ªáp ƒë√£ ƒë∆∞·ª£c t·∫£i
    return local_path  # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n c·ªßa t·ªáp t·∫£i v·ªÅ

def analyze_file(filepath):
    """
    H√†m n√†y s·∫Ω m·ªü t·ªáp CSV, t√¨m b·∫£n ghi c√≥ nhi·ªát ƒë·ªô cao nh·∫•t v√† in ra.
    """
    df = pd.read_csv(filepath)  # ƒê·ªçc t·ªáp CSV v√†o DataFrame c·ªßa Pandas

    # Ki·ªÉm tra xem c·ªôt 'HourlyDryBulbTemperature' c√≥ t·ªìn t·∫°i kh√¥ng
    if 'HourlyDryBulbTemperature' not in df.columns:
        raise Exception("'HourlyDryBulbTemperature' column not found in the file.")  # N·∫øu kh√¥ng c√≥, n√©m l·ªói

    # Chuy·ªÉn ƒë·ªïi c·ªôt 'HourlyDryBulbTemperature' th√†nh ki·ªÉu s·ªë (n·∫øu c·∫ßn)
    df['HourlyDryBulbTemperature'] = pd.to_numeric(df['HourlyDryBulbTemperature'], errors='coerce')

    # T√¨m gi√° tr·ªã nhi·ªát ƒë·ªô cao nh·∫•t
    max_temp = df['HourlyDryBulbTemperature'].max()
    # L·ªçc ra c√°c b·∫£n ghi c√≥ nhi·ªát ƒë·ªô cao nh·∫•t
    hottest_records = df[df['HourlyDryBulbTemperature'] == max_temp]

    print("\nüå° Records with the highest HourlyDryBulbTemperature:")
    print(hottest_records)  # In ra c√°c b·∫£n ghi c√≥ nhi·ªát ƒë·ªô cao nh·∫•t

def main():
    """
    H√†m ch√≠nh s·∫Ω g·ªçi c√°c h√†m tr√™n ƒë·ªÉ t√¨m ki·∫øm t·ªáp, t·∫£i t·ªáp v√† ph√¢n t√≠ch d·ªØ li·ªáu.
    """
    try:
        print("Looking for file...")  # Th√¥ng b√°o ƒëang t√¨m ki·∫øm t·ªáp
        filename = find_target_file()  # T√¨m t·ªáp v·ªõi d·∫•u th·ªùi gian c·∫ßn t√¨m

        print(f"Found file: {filename}")  # Th√¥ng b√°o t√¨m th·∫•y t·ªáp
        filepath = download_file(filename)  # T·∫£i t·ªáp v·ªÅ

        print("Analyzing file...")  # Th√¥ng b√°o ƒëang ph√¢n t√≠ch t·ªáp
        analyze_file(filepath)  # Ph√¢n t√≠ch t·ªáp ƒë·ªÉ t√¨m b·∫£n ghi c√≥ nhi·ªát ƒë·ªô cao nh·∫•t

    except Exception as e:
        print(f"Error: {e}")  # In ra l·ªói n·∫øu c√≥

# Ch·∫°y h√†m main n·∫øu t·ªáp n√†y ƒë∆∞·ª£c th·ª±c thi tr·ª±c ti·∫øp
if __name__ == "__main__":
    main()

```

> 4. Sau khi save file main.py, ch·∫°y d√≤ng l·ªánh `docker-compose up run`

> 5. K·∫øt qu·∫£ thu ƒë∆∞·ª£c
> ![8800406e25b797e9cea6](https://github.com/user-attachments/assets/3b78c261-28a4-4f6a-986c-dab636f84045)

## EXERCISE 3

> 1. Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c t·∫°i CMD th√†nh `Exercise-3`

> 2. Ch·∫°y l·ªánh docker `build --tag=exercise-3 .` ƒë·ªÉ build image Docker (Qu√° tr√¨nh di·ªÖn ra trong 2 ‚Äì 3 ph√∫t)
> ![image](https://github.com/user-attachments/assets/19b9503f-2c9b-4fcc-8476-1dfcb0a388a9)


> 3. Sau khi build xong, truy c·∫≠p file `main.py` b·∫±ng VS code

##### Code s·ª≠ d·ª•ng cho main.py:
```
import requests
import gzip
import io
import sys # ƒê·ªÉ ghi tr·ª±c ti·∫øp ra stdout, ƒë√¥i khi h·ªØu √≠ch cho streaming

# URL g·ªëc c·ªßa Common Crawl
COMMON_CRAWL_BASE_URL = 'https://data.commoncrawl.org'

def download_file(url):
    """T·∫£i t·ªáp t·ª´ URL v√† tr·∫£ v·ªÅ n·ªôi dung"""
    print(f"ƒêang t·∫£i t·ªáp t·ª´ URL: {url}")
    try:
        response = requests.get(url)
        response.raise_for_status()  # Ph√°t sinh ngo·∫°i l·ªá n·∫øu status code kh√¥ng ph·∫£i 2xx
        return response.content
    except requests.exceptions.RequestException as e:
        print(f"ƒê√£ x·∫£y ra l·ªói khi t·∫£i t·ªáp: {e}", file=sys.stderr)
        raise

def s3_uri_to_http_url(uri):
    """Chuy·ªÉn ƒë·ªïi S3 URI ho·∫∑c ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi th√†nh URL HTTP cho Common Crawl"""
    if uri.startswith('s3://'):
        s3_path = uri[len('s3://'):]
        # B·ªè qua ph·∫ßn bucket (th∆∞·ªùng l√† 'commoncrawl')
        path = s3_path[s3_path.find('/') + 1:]
        return f"{COMMON_CRAWL_BASE_URL}/{path}"
    else:
        # N·∫øu l√† ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi, s·ª≠ d·ª•ng tr·ª±c ti·∫øp
        return f"{COMMON_CRAWL_BASE_URL}/{uri}"

def main():
    # --- ƒê·ªãnh nghƒ©a c√°c tham s·ªë ---
    # URL c·ªßa t·ªáp wet.paths.gz
    WET_PATHS_URL = f"{COMMON_CRAWL_BASE_URL}/crawl-data/CC-MAIN-2022-05/wet.paths.gz"

    print(f"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu Common Crawl ---")

    try:
        # --- B∆∞·ªõc 1: T·∫£i t·ªáp .gz ban ƒë·∫ßu ---
        print(f"1. ƒêang t·∫£i t·ªáp wet.paths.gz t·ª´ {WET_PATHS_URL}")
        gz_content = download_file(WET_PATHS_URL)
        print(f"   ƒê√£ t·∫£i xong {len(gz_content)} bytes.")

        # --- B∆∞·ªõc 2: Gi·∫£i n√©n v√† ƒë·ªçc t·ªáp .gz ---
        print(f"2. ƒêang gi·∫£i n√©n v√† ƒë·ªçc d√≤ng ƒë·∫ßu ti√™n...")
        with gzip.GzipFile(fileobj=io.BytesIO(gz_content), mode='rb') as gz_file:
            wet_uri = gz_file.readline().decode('utf-8').strip()

        print(f"   D√≤ng ƒë·∫ßu ti√™n (URI t·ªáp WET) l√†: {wet_uri}")

        # --- B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi S3 URI th√†nh URL HTTP ---
        wet_url = s3_uri_to_http_url(wet_uri)
        print(f"3. URL c·ªßa t·ªáp WET: {wet_url}")

        # --- B∆∞·ªõc 4: T·∫£i t·ªáp WET v√† x·ª≠ l√Ω ---
        print(f"4. ƒêang t·∫£i t·ªáp WET...")
        wet_content = download_file(wet_url)
        print(f"   ƒê√£ t·∫£i t·ªáp WET th√†nh c√¥ng ({len(wet_content)} bytes).")

        # --- B∆∞·ªõc 5: X·ª≠ l√Ω v√† hi·ªÉn th·ªã n·ªôi dung t·ªáp WET ---
        print(f"5. N·ªôi dung c·ªßa t·ªáp WET:")
        wet_fileobj = io.BytesIO(wet_content)
        # WET files are gzipped, so we need to decompress them
        with gzip.GzipFile(fileobj=wet_fileobj, mode='rb') as wet_file:
            line_count = 0
            for line in wet_file:
                decoded_line = line.decode('utf-8', errors='replace').rstrip('\r\n')
                print(decoded_line)
                line_count += 1
                # Gi·ªõi h·∫°n s·ªë d√≤ng hi·ªÉn th·ªã ƒë·ªÉ tr√°nh qu√° nhi·ªÅu ƒë·∫ßu ra
                if line_count >= 100:
                    print("... (c√≤n nhi·ªÅu d√≤ng kh√°c)")
                    break

        print(f"   ƒê√£ hi·ªÉn th·ªã {line_count} d√≤ng t·ª´ t·ªáp WET.")
        print(f"--- Ho√†n th√†nh x·ª≠ l√Ω ---")

    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói: {e}", file=sys.stderr)
        sys.exit(1)  # Tho√°t v·ªõi m√£ l·ªói ƒë·ªÉ b√°o hi·ªáu th·∫•t b·∫°i


if __name__ == "__main__":
    main()

```
> 4. Sau khi l∆∞u file `main.py`, th·ª±c hi·ªán l·ªánh `docker-compose up run`
> 5. K·∫øt qu·∫£ sau khi th·ª±c hi·ªán
> ![image](https://github.com/user-attachments/assets/fc59b0b3-f477-43cc-9d15-0f07215786a1)



## EXERCISE-4

> 1. Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c t·∫°i CMD th√†nh `Exercise-4`

> 2. Ch·∫°y l·ªánh docker `build --tag=exercise-4 .` ƒë·ªÉ build image Docker (Qu√° tr√¨nh di·ªÖn ra trong 2 ‚Äì 3 ph√∫t)
> ![image](https://github.com/user-attachments/assets/086ebd6b-8fb8-4996-8b61-ea52b64207b7)


> 3. N·ªôi dung file `main.py`
```
import json
import csv
import glob
import os

def flatten_json(json_obj, parent_key='', sep='_'):
    """
    L√†m ph·∫≥ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON c√≥ c·∫•u tr√∫c l·ªìng nhau.
    
    Args:
        json_obj: ƒê·ªëi t∆∞·ª£ng JSON c·∫ßn l√†m ph·∫≥ng
        parent_key: Kh√≥a cha (s·ª≠ d·ª•ng ƒë·ªÉ ƒë·ªá quy)
        sep: K√Ω t·ª± ph√¢n t√°ch gi·ªØa kh√≥a cha v√† con
    
    Returns:
        dict: Dictionary ƒë√£ ƒë∆∞·ª£c l√†m ph·∫≥ng
    """
    flattened = {}
    
    # Duy·ªát qua t·∫•t c·∫£ c√°c c·∫∑p key-value trong json_obj
    for key, value in json_obj.items():
        # T·∫°o t√™n kh√≥a m·ªõi b·∫±ng c√°ch n·ªëi kh√≥a cha v√† kh√≥a hi·ªán t·∫°i
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        
        # N·∫øu gi√° tr·ªã l√† t·ª´ ƒëi·ªÉn (dict), g·ªçi ƒë·ªá quy ƒë·ªÉ l√†m ph·∫≥ng n√≥
        if isinstance(value, dict):
            # C·∫≠p nh·∫≠t flattened v·ªõi k·∫øt qu·∫£ t·ª´ h√†m ƒë·ªá quy
            flattened.update(flatten_json(value, new_key, sep))
        else:
            # N·∫øu kh√¥ng ph·∫£i dict, g√°n gi√° tr·ªã tr·ª±c ti·∫øp
            flattened[new_key] = value
    
    return flattened

def json_to_csv(json_file_path, csv_file_path):
    """
    Chuy·ªÉn ƒë·ªïi m·ªôt file JSON th√†nh file CSV.
    
    Args:
        json_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
        csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u file CSV
    """
    print(f"ƒêang chuy·ªÉn ƒë·ªïi {json_file_path} -> {csv_file_path}")
    
    # ƒê·ªçc file JSON
    with open(json_file_path, 'r') as json_file:
        json_data = json.load(json_file)
    
    # L√†m ph·∫≥ng JSON
    flattened_data = flatten_json(json_data)
    
    # L·∫•y t·∫•t c·∫£ c√°c kh√≥a ƒë·ªÉ l√†m ti√™u ƒë·ªÅ cho CSV
    fieldnames = flattened_data.keys()
    
    # Ghi ra file CSV
    with open(csv_file_path, 'w', newline='') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()  # Vi·∫øt d√≤ng ti√™u ƒë·ªÅ
        writer.writerow(flattened_data)  # Vi·∫øt d√≤ng d·ªØ li·ªáu

def main():
    """
    H√†m ch√≠nh th·ª±c hi·ªán c√°c b∆∞·ªõc x·ª≠ l√Ω:
    1. T√¨m t·∫•t c·∫£ file JSON trong th∆∞ m·ª•c data
    2. Chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh file CSV
    """
    # ƒê∆∞·ªùng d·∫´n g·ªëc ƒë·∫øn th∆∞ m·ª•c data
    data_dir = 'data'
    
    # T√¨m t·∫•t c·∫£ file .json trong data_dir v√† t·∫•t c·∫£ th∆∞ m·ª•c con c·ªßa n√≥
    # ** nghƒ©a l√† t√¨m ki·∫øm ƒë·ªá quy trong t·∫•t c·∫£ c√°c th∆∞ m·ª•c con
    # *.json nghƒ©a l√† t√¨m t·∫•t c·∫£ c√°c file c√≥ ƒëu√¥i .json
    json_files = glob.glob(os.path.join(data_dir, '**', '*.json'), recursive=True)
    
    print(f"Danh s√°ch file JSON t√¨m th·∫•y b·ªüi glob: {json_files}")
    
    print(f"ƒê√£ t√¨m th·∫•y {len(json_files)} file JSON:")
    for file in json_files:
        print(f"  - {file}")
    
    # Duy·ªát qua t·ª´ng file JSON v√† chuy·ªÉn ƒë·ªïi n√≥ th√†nh CSV
    for json_file in json_files:
        # T·∫°o t√™n file CSV t·ª´ t√™n file JSON (thay ƒëu√¥i .json th√†nh .csv)
        csv_file = json_file.replace('.json', '.csv')
        
        # G·ªçi h√†m ƒë·ªÉ chuy·ªÉn ƒë·ªïi
        json_to_csv(json_file, csv_file)
    
    print("\nHo√†n t·∫•t chuy·ªÉn ƒë·ªïi!")

if __name__ == "__main__":
    main()
```

> 4. Sau khi save file ` main.py`, th·ª±c thi l·ªánh `docker-compose up run`
> 5. K·∫øt qu·∫£ sau khi th·ª±c hi·ªán:

![image](https://github.com/user-attachments/assets/51311436-0077-4ad4-af9c-9c9cb91bae90)

![image](https://github.com/user-attachments/assets/da9d720a-35b7-4a8d-9908-ba14146c8f8e)

![image](https://github.com/user-attachments/assets/188b1474-9f76-412d-87a9-68dab9ab4110)


## EXERCISE-5

> 1.Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c t·∫°i CMD th√†nh `Exercise-5`

> 2. Ch·∫°y l·ªánh docker `build --tag=exercise-5 .` ƒë·ªÉ build image Docker
> ![image](https://github.com/user-attachments/assets/b75fd14b-9a7b-4de5-a087-e1dd36bdca41)


#### N·ªôi dung file main.py:
```
import psycopg2  # Th∆∞ vi·ªán ƒë·ªÉ k·∫øt n·ªëi v√† thao t√°c v·ªõi PostgreSQL
import csv       # ƒê·ªçc file CSV
import uuid      # Th∆∞ vi·ªán t·∫°o UUID (kh√¥ng d√πng trong ƒëo·∫°n n√†y nh∆∞ng c√≥ th·ªÉ h·ªØu √≠ch sau)
from datetime import datetime  # ƒê·ªÉ x·ª≠ l√Ω ƒë·ªãnh d·∫°ng ng√†y th√°ng
import os        # Thao t√°c v·ªõi h·ªá th·ªëng file

# H√†m t·∫°o c√°c b·∫£ng trong c∆° s·ªü d·ªØ li·ªáu n·∫øu ch∆∞a t·ªìn t·∫°i
def create_tables(cur):
    # T·∫°o b·∫£ng 'accounts'
    cur.execute("""
        CREATE TABLE IF NOT EXISTS accounts (
            customer_id INT PRIMARY KEY,
            first_name VARCHAR(50),
            last_name VARCHAR(50),
            address_1 VARCHAR(100),
            address_2 VARCHAR(100),
            city VARCHAR(50),
            state VARCHAR(20),
            zip_code VARCHAR(10),
            join_date DATE
        );
    """)
    # T·∫°o index cho tr∆∞·ªùng 'last_name' ƒë·ªÉ tƒÉng t·ªëc truy v·∫•n
    cur.execute("CREATE INDEX IF NOT EXISTS idx_accounts_last_name ON accounts(last_name);")

    # T·∫°o b·∫£ng 'products'
    cur.execute("""
        CREATE TABLE IF NOT EXISTS products (
            product_id INT PRIMARY KEY,
            product_code INT,
            product_description VARCHAR(100)
        );
    """)
    # T·∫°o index cho tr∆∞·ªùng 'product_code'
    cur.execute("CREATE INDEX IF NOT EXISTS idx_products_code ON products(product_code);")

    # T·∫°o b·∫£ng 'transactions'
    cur.execute("""
        CREATE TABLE IF NOT EXISTS transactions (
            transaction_id TEXT PRIMARY KEY,
            transaction_date DATE,
            product_id INT REFERENCES products(product_id),
            product_code INT,
            product_description VARCHAR(100),
            quantity INT,
            account_id INT REFERENCES accounts(customer_id)
        );
    """)
    # T·∫°o index ƒë·ªÉ t·ªëi ∆∞u truy v·∫•n tr√™n product_id v√† account_id
    cur.execute("CREATE INDEX IF NOT EXISTS idx_transactions_product_id ON transactions(product_id);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_transactions_account_id ON transactions(account_id);")


# H√†m nh·∫≠p d·ªØ li·ªáu t·ª´ c√°c file CSV v√†o database
def import_csv(cur, conn):
    base_path = os.path.join(os.path.dirname(__file__), "data")  # ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c 'data'

    # Nh·∫≠p d·ªØ li·ªáu cho b·∫£ng 'accounts'
    with open(os.path.join(base_path, "accounts.csv"), newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cur.execute("""
                INSERT INTO accounts (
                    customer_id, first_name, last_name, address_1, address_2,
                    city, state, zip_code, join_date
                ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)
                ON CONFLICT (customer_id) DO NOTHING;  -- N·∫øu tr√πng kh√≥a ch√≠nh th√¨ b·ªè qua
            """, (
                int(row["customer_id"]),
                row["first_name"],
                row["last_name"],
                row["address_1"],
                row["address_2"] if row["address_2"] != "NaN" else None,  # Chuy·ªÉn 'NaN' th√†nh None
                row["city"],
                row["state"],
                row["zip_code"],
                datetime.strptime(row["join_date"], "%Y/%m/%d").date()  # Chuy·ªÉn chu·ªói th√†nh ki·ªÉu DATE
            ))
        conn.commit()  # L∆∞u thay ƒë·ªïi sau khi ch√®n xong

    # Nh·∫≠p d·ªØ li·ªáu cho b·∫£ng 'products'
    with open(os.path.join(base_path, "products.csv"), newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cur.execute("""
                INSERT INTO products (
                    product_id, product_code, product_description
                ) VALUES (%s, %s, %s)
                ON CONFLICT (product_id) DO NOTHING;
            """, (
                int(row["product_id"]),
                int(row["product_code"]),
                row["product_description"]
            ))
        conn.commit()

    # Nh·∫≠p d·ªØ li·ªáu cho b·∫£ng 'transactions'
    with open(os.path.join(base_path, "transactions.csv"), newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cur.execute("""
                INSERT INTO transactions (
                    transaction_id, transaction_date, product_id, product_code,
                    product_description, quantity, account_id
                ) VALUES (%s,%s,%s,%s,%s,%s,%s)
                ON CONFLICT (transaction_id) DO NOTHING;
            """, (
                row["transaction_id"],
                datetime.strptime(row["transaction_date"], "%Y/%m/%d").date(),
                int(row["product_id"]),
                int(row["product_code"]),
                row["product_description"],
                int(row["quantity"]),
                int(row["account_id"])
            ))
        conn.commit()


# H√†m ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô ch∆∞∆°ng tr√¨nh
def main():
    # Th√¥ng tin k·∫øt n·ªëi t·ªõi database
    host = "postgres"
    database = "postgres"
    user = "postgres"
    pas = "postgres"
    conn = psycopg2.connect(host=host, database=database, user=user, password=pas)
    cur = conn.cursor()

    # T·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
    create_tables(cur)
    conn.commit()

    # Nh·∫≠p d·ªØ li·ªáu t·ª´ CSV
    import_csv(cur, conn)

    print("‚úÖ Tables created and CSV data imported successfully.")

    # ƒê√≥ng k·∫øt n·ªëi
    cur.close()
    conn.close()


# Khi ch·∫°y file tr·ª±c ti·∫øp th√¨ th·ª±c hi·ªán h√†m main()
if __name__ == "__main__":
    main()

```

> 3. Sau khi l∆∞u l·∫°i, th·ª±c thi l·ªánh `docker-compose up run`
> 4. K·∫øt qu·∫£ sau khi th·ª±c hi·ªán:
![image](https://github.com/user-attachments/assets/4625ce20-fb40-49e9-b9c0-be0f6bb9acbf)

> Truy v·∫•n c√°c b·∫£ng v·ª´a t·∫°o trong container posgres-1
![image](https://github.com/user-attachments/assets/e70ea051-15fc-4f1b-9cbd-0e3e85bf7830)

## PIPELINE T·ª∞ ƒê·ªòNG TH·ª∞C HI·ªÜN B√ÄI T·∫¨P 1- 5
#### Code cho pipeline.py:
```
import os
import subprocess

# List c√°c Exercise b·∫°n mu·ªën ch·∫°y
exercises = ['Exercise-1', 'Exercise-2', 'Exercise-3', 'Exercise-4', 'Exercise-5']

# H√†m ki·ªÉm tra xem image ƒë√£ c√≥ ch∆∞a
def check_image_exists(image_name):
    result = subprocess.run(['docker', 'images', '-q', image_name], stdout=subprocess.PIPE)
    return result.stdout.decode().strip() != ''

# H√†m build image n·∫øu ch∆∞a c√≥
def build_image(exercise_name):
    print(f"Image {exercise_name} ch∆∞a c√≥, b·∫Øt ƒë·∫ßu build...")
    result = subprocess.run(['docker', 'build', '-t', exercise_name, f'D:/NMKTDL/data-engineering-practice-main/Exercises/{exercise_name}'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        print(f"Build image {exercise_name} th·∫•t b·∫°i. D·ª´ng pipeline.")
        print(result.stderr.decode())
        exit(1)
    print(f"Build image {exercise_name} th√†nh c√¥ng.")

# H√†m ch·∫°y docker-compose
def run_docker_compose(exercise_name):
    print(f"ƒêang ch·∫°y {exercise_name} b·∫±ng image {exercise_name}...")
    compose_file = f'D:/NMKTDL/data-engineering-practice-main/Exercises/{exercise_name}/docker-compose.yml'
    result = subprocess.run(['docker-compose', '-f', compose_file, 'up'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        print(f"L·ªói khi ch·∫°y {exercise_name}. D·ª´ng pipeline.")
        print(result.stderr.decode())
        exit(1)
    
    # Ki·ªÉm tra logs c·ªßa c√°c container
    print("Ki·ªÉm tra logs c·ªßa c√°c container...")
    logs_result = subprocess.run(['docker-compose', '-f', compose_file, 'logs'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print(logs_result.stdout.decode())  # In logs ƒë·ªÉ xem chi ti·∫øt

    print(f"{exercise_name} ƒë√£ ho√†n t·∫•t!")

# Pipeline
def run_pipeline():
    for exercise in exercises:
        # Ki·ªÉm tra image ƒë√£ t·ªìn t·∫°i ch∆∞a
        if not check_image_exists(exercise.lower()):
            build_image(exercise)
        else:
            print(f"Image {exercise} ƒë√£ c√≥ s·∫µn.")
        
        # Ch·∫°y docker-compose cho b√†i
        run_docker_compose(exercise)

    print("\nPipeline ho√†n t·∫•t!")

if __name__ == "__main__":
    run_pipeline()
```
> #### K·∫øt qu·∫£ th·ª±c hi·ªán:
> ##### Exercise-1
> ![image](https://github.com/user-attachments/assets/63d6cecb-88f2-48c4-816d-1dfc4b767f61)
> ##### Exercise-2 & 3
> ![image](https://github.com/user-attachments/assets/fedd1e65-6da1-4dc1-b9c3-ad1e2b8f21a3)
> ##### Exercise-4
> ![image](https://github.com/user-attachments/assets/71196633-75a3-4ead-b33b-21cdd6eafd2f)
> 

### C·∫§U TR√öC TH∆Ø M·ª§C ƒê·ªÇ CH·∫†Y DAG AIRFLOW
```
data-engineering-practice-Le_Trung_Huu/
‚îÇ
‚îú‚îÄ‚îÄ dags/                    ‚Üê üìÇ Ch·ª©a pipeline_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_dag.py  
‚îÇ
‚îú‚îÄ‚îÄ Exercises/                   ‚Üê üìÇ Ch·ª©a c√°c b√†i t·∫≠p v·ªõi Dockerfile ri√™ng
‚îÇ   ‚îú‚îÄ‚îÄ Exercise-1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py 
‚îÇ   ‚îú‚îÄ‚îÄ Exercise-2/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py 
‚îÇ   ‚îú‚îÄ‚îÄ Exercise-3/
‚îÇ   ‚îú‚îÄ‚îÄ Exercise-4/
‚îÇ   ‚îî‚îÄ‚îÄ Exercise-5/
‚îÇ
‚îú‚îÄ‚îÄ pipeline.py                  ‚Üê üìÑ Code pipeline g·ªëc n·∫øu mu·ªën g·ªçi ngo√†i Airflow
‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê ‚úÖ Ch·∫°y trong th∆∞ m·ª•c n√†y
‚îî‚îÄ‚îÄ Dockerfile
```
#### Dockerfile
```
FROM python:3.10-slim

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
RUN pip install --no-cache-dir -r requirements.txt

# Sao ch√©p m√£ ngu·ªìn t·ª´ th∆∞ m·ª•c hi·ªán t·∫°i v√†o trong container
COPY . /app

WORKDIR /app

# L·ªánh ch·∫°y khi container ƒë∆∞·ª£c kh·ªüi ƒë·ªông
CMD ["python", "main.py"]
```
#### Docker-compose:
```
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - app-network

  airflow-init:
    image: apache/airflow:2.9.1-python3.10
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: bash -c "
      airflow db init && \
      airflow users create \
        --username airflow \
        --password airflow \
        --firstname Air \
        --lastname Flow \
        --role Admin \
        --email airflow@example.com
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./Exercises:/opt/airflow/Exercises  # Mount th∆∞ m·ª•c Exercises t·ª´ ngo√†i v√†o container
    networks:
      - app-network

  airflow-webserver:
    image: apache/airflow:2.9.1-python3.10
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'True'
    ports:
      - "8080:8080"
    command: ["airflow", "webserver"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./Exercises:/opt/airflow/Exercises  # Mount th∆∞ m·ª•c Exercises t·ª´ ngo√†i v√†o container
    networks:
      - app-network

  airflow-scheduler:
    image: apache/airflow:2.9.1-python3.10
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: ["airflow", "scheduler"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./Exercises:/opt/airflow/Exercises  # Mount th∆∞ m·ª•c Exercises t·ª´ ngo√†i v√†o container
    networks:
      - app-network

  pgadmin:
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - app-network

volumes:
  postgres-db-volume:

networks:
  app-network:
    driver: bridge

#docker-compose up airflow-init
```

#### Pipeline-dag.py
```
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import subprocess

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

def run_main_py(path):
    print(f"üîÅ ƒêang ch·∫°y: {path}")
    result = subprocess.run(["python", path], capture_output=True, text=True)
    print("üìÑ STDOUT:", result.stdout)
    print("‚ùó STDERR:", result.stderr)
    result.check_returncode()

with DAG(
    dag_id='exercise_main_pipeline',
    default_args=default_args,
    description='Ch·∫°y t·∫•t c·∫£ c√°c main.py trong m·ªói Exercise h√†ng ng√†y l√∫c 10h s√°ng',
    schedule_interval='0 10 * * *',  # 10:00 UTC m·ªói ng√†y
    start_date=datetime(2025, 4, 25),
    catchup=False,
    tags=["exercise"],
) as dag:

    exercises_dir = "/opt/airflow/Exercises"

    if not os.path.exists(exercises_dir):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {exercises_dir}")

    previous_task = None

    for ex in sorted(os.listdir(exercises_dir)):
        ex_path = os.path.join(exercises_dir, ex, "main.py")
        if os.path.isfile(ex_path):
            task = PythonOperator(
                task_id=f'run_{ex.lower()}',
                python_callable=run_main_py,
                op_args=[ex_path],
            )
            if previous_task:
                previous_task >> task
            previous_task = task
```

### K·∫æT QU·∫¢ SAU KHI CH·∫†Y DAG
>![image](https://github.com/user-attachments/assets/749ac8e5-5aea-428b-b2b4-df0cf866040c)

> ![image](https://github.com/user-attachments/assets/b889a380-7201-49db-af30-2384068a9653)
>![image](https://github.com/user-attachments/assets/5c9d74f3-9391-4b29-8a4a-6bc10718b474)

> # REPORT - LAB 8:
## EXERCISE 1

> 1. Th·ª±c thi l·ªánh sau trong CMD: git clone ƒë·ªÉ clone GitHub repo v·ªÅ m√°y c·ªßa m√¨nh
> ![649dabe12114934aca05](https://github.com/user-attachments/assets/d4b2284b-1f99-4733-95e8-388c142eeb00)


> 2. Sau ƒë√≥ ti·∫øn h√†nh ch·∫°y l·ªánh `cd cd Build-data-warehouse-with-Airflow-Python-for-E-commerce

> 3. Ti·∫øp t·ª•c th·ª±c hi·ªán l·ªánh: 'docker-compose up -d' ƒë·ªÉ build Docker image Qu√° tr√¨nh s·∫Ω m·∫•t v√†i ph√∫t
> ![Annotation 2025-05-06 211724](https://github.com/user-attachments/assets/44cda780-fdd2-4535-8973-e4596bcdad04)

> 4. M·ªü Airflow Web UI ƒë·ªÉ xem qu√° tr√¨nh ch·∫°y
> M·∫∑c ƒë·ªãnh, Airflow Web UI s·∫Ω ch·∫°y t·∫°i: http://localhost:8080
> ![Annotation 2025-05-06 215312](https://github.com/user-attachments/assets/81a636bf-8c9c-492a-a5f3-bab8aeffe751)
> ![image](https://github.com/user-attachments/assets/93b7b823-de6b-4931-9f7b-133ba179dcf9)
> 5. Check l·∫°i trong dbeaver
> ![image](https://github.com/user-attachments/assets/733cbed6-c76c-44fa-8223-4d9a6f936319)
## EXERCISE 2

> 1. Chu·∫©n b·ªã m√¥i tr∆∞·ªùng l√†m vi·ªác:

T·∫°o m·ªôt th∆∞ m·ª•c d·ª± √°n ri√™ng bi·ªát tr√™n h·ªá th·ªëng m√°y t√≠nh ƒë·ªÉ ch·ª©a t·∫•t c·∫£ c√°c file c·∫•u h√¨nh v√† m√£ ngu·ªìn li√™n quan ƒë·∫øn Airflow.

B√™n trong th∆∞ m·ª•c d·ª± √°n, t·∫°o m·ªôt th∆∞ m·ª•c con c√≥ t√™n l√† dags. ƒê√¢y l√† n∆°i s·∫Ω ch·ª©a c√°c file ƒë·ªãnh nghƒ©a DAG c·ªßa Airflow (c√°c file .py).

ƒê·∫∑t c√°c file ƒë·ªãnh nghƒ©a DAG ƒë√£ c√≥ s·∫µn (simple_dag_local.py, complex_dag_local.py, miai_dag.py, sensor_local.py) v√†o th∆∞ m·ª•c dags v·ª´a t·∫°o.
![image](https://github.com/user-attachments/assets/a7fecc65-8c3d-4927-8225-db3aaef9fc20)


> 2. X√¢y d·ª±ng file c·∫•u h√¨nh Docker Compose (docker-compose.yaml):

T·∫°o file docker-compose.yaml ·ªü th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n.

C·∫•u h√¨nh c√°c d·ªãch v·ª• c·∫ßn thi·∫øt ƒë·ªÉ ch·∫°y Airflow:

D·ªãch v·ª• C∆° s·ªü d·ªØ li·ªáu (PostgreSQL): ƒê·ªãnh nghƒ©a m·ªôt d·ªãch v·ª• s·ª≠ d·ª•ng image PostgreSQL ƒë·ªÉ l√†m c∆° s·ªü d·ªØ li·ªáu l∆∞u tr·ªØ metadata c·ªßa Airflow. C·∫•u h√¨nh t√™n database, ng∆∞·ªùi d√πng v√† m·∫≠t kh·∫©u ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa Airflow. S·ª≠ d·ª•ng Docker Volume ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu database ƒë∆∞·ª£c l∆∞u tr·ªØ b·ªÅn v·ªØng.

D·ªãch v·ª• Airflow: ƒê·ªãnh nghƒ©a m·ªôt d·ªãch v·ª• s·ª≠ d·ª•ng image Apache Airflow. C·∫•u h√¨nh d·ªãch v·ª• n√†y ƒë·ªÉ ph·ª• thu·ªôc v√†o d·ªãch v·ª• c∆° s·ªü d·ªØ li·ªáu, ƒë·∫£m b·∫£o database s·∫µn s√†ng tr∆∞·ªõc khi Airflow kh·ªüi ƒë·ªông. √Ånh x·∫° (mount) th∆∞ m·ª•c dags t·ª´ m√°y host v√†o th∆∞ m·ª•c DAGs b√™n trong container Airflow. C·∫•u h√¨nh c·ªïng (port) ƒë·ªÉ truy c·∫≠p giao di·ªán web c·ªßa Airflow t·ª´ m√°y host (m·∫∑c ƒë·ªãnh l√† 8080).
code file docker-compose.yml:
```
# S·ª≠ d·ª•ng phi√™n b·∫£n Docker Compose
# B·∫°n c√≥ th·ªÉ x√≥a d√≤ng version: '3' ƒë·ªÉ tr√°nh c·∫£nh b√°o obsolete n·∫øu mu·ªën
version: '3'

# ƒê·ªãnh nghƒ©a c√°c d·ªãch v·ª• (services)
services:
  # D·ªãch v·ª• c∆° s·ªü d·ªØ li·ªáu PostgreSQL cho Airflow metadata
  postgres:
    image: postgres:14 # S·ª≠ d·ª•ng image PostgreSQL phi√™n b·∫£n 14
    environment:
      # C·∫•u h√¨nh th√¥ng tin ƒëƒÉng nh·∫≠p v√† t√™n database cho PostgreSQL
      # Airflow s·∫Ω s·ª≠ d·ª•ng database c√≥ t√™n 'airflow'
      - POSTGRES_USER=airflow # <-- T√™n ng∆∞·ªùi d√πng database (c√≥ th·ªÉ ƒë·ªïi)
      - POSTGRES_PASSWORD=airflow # <-- M·∫≠t kh·∫©u database (c√≥ th·ªÉ ƒë·ªïi)
      - POSTGRES_DB=airflow # <-- T√™n database m√† Airflow s·∫Ω k·∫øt n·ªëi
    volumes:
      # L∆∞u tr·ªØ d·ªØ li·ªáu c·ªßa database v√†o m·ªôt volume ƒë·ªÉ d·ªØ li·ªáu kh√¥ng b·ªã m·∫•t khi container d·ª´ng/x√≥a
      - postgres_data:/var/lib/postgresql/data
    ports:
      # √Ånh x·∫° c·ªïng 5432 c·ªßa container ra c·ªïng 5432 tr√™n m√°y host (t√πy ch·ªçn, h·ªØu √≠ch cho vi·ªác debug database)
      - "5432:5432"
    networks:
      # K·∫øt n·ªëi d·ªãch v·ª• n√†y v√†o m·∫°ng n·ªôi b·ªô 'airflow-network'
      - airflow-network
    healthcheck: # Ki·ªÉm tra t√¨nh tr·∫°ng s·ª©c kh·ªèe c·ªßa database
      test: [ "CMD", "pg_isready", "-U", "airflow" ] # L·ªánh ki·ªÉm tra
      interval: 5s # Ki·ªÉm tra m·ªói 5 gi√¢y
      retries: 5 # Th·ª≠ l·∫°i 5 l·∫ßn n·∫øu l·ªói

  # D·ªãch v·ª• Airflow
  airflow:
    image: apache/airflow:2.6.3 # S·ª≠ d·ª•ng image Airflow phi√™n b·∫£n 2.6.3
    depends_on:
      # Airflow ph·ª• thu·ªôc v√†o database, ƒë·∫£m b·∫£o postgres kh·ªüi ƒë·ªông tr∆∞·ªõc
      postgres:
        condition: service_healthy # Ch·ªù cho d·ªãch v·ª• postgres ·ªü tr·∫°ng th√°i healthy
    volumes:
      # √Ånh x·∫° th∆∞ m·ª•c 'dags' tr√™n m√°y host v√†o th∆∞ m·ª•c DAGs b√™n trong container Airflow
      # ƒê√¢y l√† n∆°i b·∫°n s·∫Ω ƒë·∫∑t c√°c file DAG c·ªßa m√¨nh
      - ./dags:/opt/airflow/dags # <-- Th∆∞ m·ª•c DAGs tr√™n host : Th∆∞ m·ª•c DAGs trong container
      # √Ånh x·∫° c√°c th∆∞ m·ª•c kh√°c n·∫øu DAG c·ªßa b·∫°n c·∫ßn ƒë·ªçc/ghi file ·ªü ngo√†i th∆∞ m·ª•c DAGs
      # V√≠ d·ª•:
      # - ./data:/opt/airflow/data # Th∆∞ m·ª•c data cho Complex DAG
      # - ./stock_data:/opt/airflow/stock_data # Th∆∞ m·ª•c stock_data cho ML DAG
      # - ./models:/opt/airflow/models # Th∆∞ m·ª•c models cho ML DAG
      # - ./data_in:/opt/airflow/data_in # Th∆∞ m·ª•c input cho Sensor DAG
    ports:
      # √Ånh x·∫° c·ªïng 8080 c·ªßa webserver Airflow ra c·ªïng 8080 tr√™n m√°y host
      - "8080:8080" # <-- C·ªïng tr√™n host : C·ªïng webserver trong container
    command: standalone # Ch·∫°y Airflow ·ªü ch·∫ø ƒë·ªô standalone (bao g·ªìm webserver v√† scheduler)
    environment:
      # C·∫•u h√¨nh k·∫øt n·ªëi database cho Airflow
      # ƒê·∫£m b·∫£o th√¥ng tin n√†y kh·ªõp v·ªõi th√¥ng tin trong service postgres
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow # <-- Th√¥ng tin k·∫øt n·ªëi DB
      - AIRFLOW__CORE__LOAD_EXAMPLES=False # Kh√¥ng t·∫£i c√°c DAG v√≠ d·ª• ƒëi k√®m Airflow
      # C·∫•u h√¨nh m√∫i gi·ªù (t√πy ch·ªçn)
      # - AIRFLOW__CFG__CORE__DEFAULT_TIMEZONE=Asia/Ho_Chi_Minh # <-- ƒê·ªïi m√∫i gi·ªù n·∫øu c·∫ßn

      # TƒÉng th·ªùi gian timeout cho Gunicorn webserver
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=300 # <-- TH√äM ho·∫∑c S·ª¨A d√≤ng n√†y, tƒÉng timeout l√™n 300 gi√¢y (5 ph√∫t)

    networks:
      # K·∫øt n·ªëi d·ªãch v·ª• n√†y v√†o m·∫°ng n·ªôi b·ªô 'airflow-network'
      - airflow-network
    healthcheck: # Ki·ªÉm tra t√¨nh tr·∫°ng s·ª©c kh·ªèe c·ªßa Airflow webserver
      test: [ "CMD", "curl", "--fail", "-s", "http://localhost:8080/health" ] # L·ªánh ki·ªÉm tra
      interval: 30s # Ki·ªÉm tra m·ªói 30 gi√¢y
      timeout: 30s # Timeout sau 30 gi√¢y
      retries: 5 # Th·ª≠ l·∫°i 5 l·∫ßn n·∫øu l·ªói

# ƒê·ªãnh nghƒ©a m·∫°ng n·ªôi b·ªô cho c√°c d·ªãch v·ª•
networks:
  airflow-network:
    driver: bridge # S·ª≠ d·ª•ng driver m·∫°ng bridge

# ƒê·ªãnh nghƒ©a c√°c volume ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu b·ªÅn v·ªØng
volumes:
  postgres_data: # Volume cho d·ªØ li·ªáu PostgreSQL

```

> 3. T·∫°o Dockerfile t√πy ch·ªânh ƒë·ªÉ c√†i ƒë·∫∑t Dependencies:

T·∫°o file Dockerfile ·ªü th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n.

S·ª≠ d·ª•ng image Apache Airflow g·ªëc l√†m n·ªÅn (FROM apache/airflow:...).

Th√™m c√°c l·ªánh RUN pip install ƒë·ªÉ c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán Python b·ªï sung m√† c√°c file DAG ph·ª©c t·∫°p c·∫ßn (v√≠ d·ª•: pymysql, pandas, sendgrid, scikit-learn, tensorflow). ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c DAG c√≥ th·ªÉ ƒë∆∞·ª£c import v√† ch·∫°y m√† kh√¥ng g·∫∑p l·ªói thi·∫øu th∆∞ vi·ªán.

Ch·ªânh s·ª≠a file docker-compose.yaml ƒë·ªÉ d·ªãch v·ª• Airflow s·ª≠ d·ª•ng build: . thay v√¨ image: ..., ch·ªâ ƒë·ªãnh Docker Compose x√¢y d·ª±ng image t·ª´ Dockerfile n√†y.
code dockerfile:
```
# S·ª≠ d·ª•ng image Airflow g·ªëc l√†m n·ªÅn
FROM apache/airflow:2.6.3

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán Python b·ªï sung m√† c√°c DAG c·ªßa b·∫°n c·∫ßn
# D·ª±a tr√™n c√°c file DAG b·∫°n cung c·∫•p, ch√∫ng ta c·∫ßn pymysql, pandas, sendgrid
# B·∫°n c√≥ th·ªÉ th√™m c√°c th∆∞ vi·ªán kh√°c v√†o ƒë√¢y n·∫øu DAG c·ªßa b·∫°n c·∫ßn
RUN pip install --no-cache-dir \
    pymysql \
    pandas \
    sendgrid \
    scikit-learn \
    tensorflow
    # Th√™m c√°c th∆∞ vi·ªán kh√°c n·∫øu c·∫ßn, v√≠ d·ª•:
    # scikit-learn \ # Cho miai_dag.py
    # tensorflow # Cho miai_dag.py

```

> 4. Kh·ªüi ƒë·ªông m√¥i tr∆∞·ªùng Airflow:

M·ªü terminal ho·∫∑c command prompt v√† ƒëi·ªÅu h∆∞·ªõng ƒë·∫øn th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n.

Ch·∫°y l·ªánh docker compose up -d --build. L·ªánh n√†y s·∫Ω:

Build image Docker t√πy ch·ªânh cho d·ªãch v·ª• Airflow (bao g·ªìm c·∫£ vi·ªác c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán b·ªï sung).

T·∫£i image PostgreSQL (n·∫øu ch∆∞a c√≥).

T·∫°o v√† kh·ªüi ƒë·ªông c√°c container cho d·ªãch v·ª• c∆° s·ªü d·ªØ li·ªáu v√† Airflow ·ªü ch·∫ø ƒë·ªô n·ªÅn.

T·∫°o ng∆∞·ªùi d√πng qu·∫£n tr·ªã ƒë·∫ßu ti√™n:

Sau khi c√°c container kh·ªüi ƒë·ªông v√† d·ªãch v·ª• Airflow ƒë√£ s·∫µn s√†ng (c√≥ th·ªÉ m·∫•t v√†i ph√∫t), s·ª≠ d·ª•ng l·ªánh docker compose exec ƒë·ªÉ ch·∫°y l·ªánh Airflow CLI b√™n trong container Airflow nh·∫±m t·∫°o ng∆∞·ªùi d√πng qu·∫£n tr·ªã ƒë·∫ßu ti√™n. L·ªánh c√≥ d·∫°ng: docker compose exec airflow bash -c "airflow users create --username <t√™n> --password <m·∫≠t kh·∫©u> --firstname <t√™n> --lastname <h·ªç> --email <email> --role Admin".
![image](https://github.com/user-attachments/assets/35d865b8-644f-423d-83ae-ad476c307d85)
![image](https://github.com/user-attachments/assets/a107abdb-486b-4029-b60e-3cc07cdbbab0)
![image](https://github.com/user-attachments/assets/10fc7c4e-a329-450b-b8d7-8003a177d087)


> 5. Truy c·∫≠p giao di·ªán web v√† k√≠ch ho·∫°t DAGs:

M·ªü tr√¨nh duy·ªát web v√† truy c·∫≠p ƒë·ªãa ch·ªâ http://localhost:8080.

ƒêƒÉng nh·∫≠p b·∫±ng th√¥ng tin t√†i kho·∫£n qu·∫£n tr·ªã v·ª´a t·∫°o.

Tr√™n giao di·ªán web, ki·ªÉm tra danh s√°ch c√°c DAG. C√°c DAG ƒë√£ copy v√†o th∆∞ m·ª•c dags s·∫Ω xu·∫•t hi·ªán.

K√≠ch ho·∫°t (Unpause) c√°c DAG mong mu·ªën b·∫±ng c√°ch nh·∫•n v√†o n√∫t g·∫°t b√™n c·∫°nh t√™n DAG.
![image](https://github.com/user-attachments/assets/270c0a38-4637-4f1b-8a97-50c50608cf6e)

